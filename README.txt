# Real-Estate-Price-Prediction:

    # Description:
        This is a project about scrapping data from the Immoweb.be website and sorting them into a csv file.
        (Because of some bug generated by python, some price data are wrongs (billons â‚¬). Don't forget to filter the datas later !)

    # Installation:
        PART 1 - DATA AQUISITION

        PART 2 - DATA ANALYSIS

        PART 3 - MODEL TRAINING

        PART 4 - API
            Take the whole API file and use it to create a container. 
    # Usage: 
        The data recolted are not used for any commercial activities.

        PART 1 - DATA AQUISITION
            run the main file from the REAL-ESTATE-PRICE-PREDICTION folder

        PART 2 - DATA ANALYSIS

        PART 3 - MODEL TRAINING

        PART 4 - API
            Get to the url from the API and the /home directory. 
            You will get access to a html webforms where you can input your datas and get a prediction by pushing the submit button

            You can also use it in your code with a GET request at the url of the API
            image.png

    # Visuals:
        Terminal
        HTML page

    # Contributors:
        ----Part 1----
        Andy Gilet
        Cyril Verwimp
        Ibrahim Mettioui
        ----Part 2----
        Andy Gilet
        ----Part 3----
        Andy Gilet
        ----Part 4----
        Andy Gilet

    # Timeline:
        ----Part 1----
        We had a week for this project (5 work day).
        Day 1:
            Andy is working on scrapping the links from the Immoweb search pages with selenium.
            Cyril is working on getting the data from a Immoweb property page.
            Ibrahim is sick.
        Day 2:
            Andy is finishing the scrapper for the links.
            Cyril is still collecting data and has started to clean them a bit.
            Ibrahim is cleaning data and reading the work from the previous day.
        Day 3:
            Andy has finished the scrapper for links and got 68k links in a csv file. He started to work on multi-threading the task.
            Cyril has finished cleaning the data and put the final data in a csv file.
            Ibrahim helped clean the data.
        Day 4:
            Andy is documenting his functions and writing the Readme.
            Cyril is documenting his functions.
            The project is finished.
        Day 5:
            Imporving the code and cheeking for bugs

        ----Part 2----
        I (Andy Gilet) had seven days for this part of the project
        Day 1:
            Hum, ok let's look at what the others have done.
            one hour later - WTF ?
        Day 2:
            Still working on redoing what Cyril has done. Why can't he comment his code correctly and make smaller functions !
        Day 3:
            This night, I had a dream. I was strangling Cyril in his sleep. What a sweet dream.
            After a day of working in this mess, I've decided to say that his code and the data he put together was ok and start working on it the day after.
        Day 4:
            I searched and created two new data set for the demography in belgium and the crime rates.
            I'm poundering if I should make a database online to had complexity to the project.
        Day 5:
            Still working on the dataFrame for the plots.
        Day 6:
            The dataframes for the plots are finished and the plots are finished to.
            I still have to make small adjustments to make them more readable.
        Day 7:
            I put my plots on the presentation, comments my functions, and updated the readme

        ----Part 3----
        I (Andy Gilet) had six days for this part of the project
        Day 1: started to learn about machine learning
        Day 2: doing the drills a bit to understand what I learned during day 1
        Day 3: started doing some test with a few models
        Day 4: On sick leave
        Day 5: On sick leave
        Day 6: ON sick leave

        ----Part 4----
        I (Andy Gilet) had height days for this part of the project
        Day 1: Rushed to train a bunch of models. The best ones were 65% and 49% accurate. I will improve them if I have enough time.
        Day 2: Learned to use Flask and implemented it as a rest API to use it with a GET request.
        Day 3: I corrected a few bugs from flask and started working on the datas
        Day 4: 
        Day 5: Found a interesting and more user-friendly way to request a estimate from the API using webforms.
        Day 6: I have finished implementing the interface to make request to the API with a web form.
               I still have to clean the data I got with the two methods before I give it to my model.
        Day 7: Used Pickle to import my model and include it in the API. updated the README file. clean the data given to the model (not finished)
    # Personnal situation:
        ----Part 1----
        Andy Gilet:
            I did not encounter any problems

        Cyril Verwimp:
            I made the data_analyse and cleaning part ! Ibrahim helped me with the cleaning !
            The project was really intersting and challenging ! And I liked it a lot

        Ibrahim Mettioui:
           I struggle with the scrapping but thanks to Andy and Cyril for helping me to clarify and to contribute to the project
        ----Part 2----
        Andy Gilet:
            It took me a bit of time because I was a bit obsessed with redoing a part of Cyril work I had a bit of a struggle to understand.
            After 3 days of frustration, I said screw it and decided that the dataset Cyril put together was probably ok and start my project from that point.
        ----Part 3 ----
        Andy Gilet:
            I had to take a sick leave the last 3 days of the project so it is not finished at all
        ----Part 4----
        Andy Gilet:
            I had to rushed a bit things here since I had to catch up with the model that I did not finished during part 3 since I was sick.
            I was not able to finish everything on time but I think creating the containeur and putting everything online will take me one more day max.